{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75a9db55",
   "metadata": {},
   "source": [
    "# Document and Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d7c1e4",
   "metadata": {},
   "source": [
    "In this notebook we outline the process of creating vector representations of the text in the wikipedia articles as well as their node in the network. The best way to interact and understand the embeddings is in [`tensorboard`](https://www.tensorflow.org/tensorboard) and we have hosted the data on the following two links bellow for you to play around with. We recommend testing both PCA and UMAP for dimensionality reduction.\n",
    "\n",
    "* [Document Embeddings](https://projector.tensorflow.org/?config=https://gist.githubusercontent.com/MatPiq/139f3fccd6f0d0c6c9077f3aa87bd301/raw/0ab01b0d1615ac1f8ff686775fb9afa8b43e5422/config.json)\n",
    "* [Node Embeddings]\n",
    "\n",
    "The rest of this page is structured as follows: first, we compute the document embeddings and give a brief explanation of the method. Next, we do the same for node embeddings. We finish with an analysis where we look at the similarities between the document and node embeddings by computing the correlation coefficient between their correponding principal component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "064bd42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from collections import namedtuple\n",
    "from node2vec import Node2Vec\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "data = pd.read_csv('wiki_df.csv.gz')\n",
    "#warnings.warn(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48efb2c9",
   "metadata": {},
   "source": [
    "## Document Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f02763c",
   "metadata": {},
   "source": [
    "Document embeddings is an extension of `word2vec` which allows us to estimate a vectorial representation of documents using shallow neural networks {cite}`le2014distribute, mikolov2013efficien`. In our case the documents are represented by the wikipedia articles corresponding to the social science disciplines. The task of the network is to predict a word $x_k$ based on a defined amount of surrounding words $x_{k-c}$ and $x_{k+c}$ called the context, where $c$ is the size of the window. While this task is not very interesting in itself, it forces the hidden layer to learn a numerical representation of the words that takes context into account. By also including an indicator variable $x_{ck}$ for each document we simultaniously learn the representation of documents in the same latent vector space. In a paper by by {citep}`rheault2020word` for example, they showed that one can extract meaningful representations of the ideology of politicians and parties using a parliamentary corpora. The gif bellow is borrowed from https://github.com/tsandefer/dsi_capstone_2 and shows the a simplified visualization of the architecture. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6bcace8",
   "metadata": {},
   "source": [
    "![gid](https://raw.githubusercontent.com/tsandefer/dsi_capstone_2/master/images/model_path.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c313b1",
   "metadata": {},
   "source": [
    "To run `doc2vec` we first need to create a list of documents. Each document is a `namedtuple` containing the text and the indicator variable for the article. We also set several hypyerparameters, the most important being the dimensions of the hidden layer `vector_size`. Since the data is relatively small we set this 64, common for larger corpuses being in the range of 200-300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72304d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "#Define a document data obj\n",
    "document_tup = namedtuple('Doc', 'words, tags')\n",
    "for row in data.iterrows():\n",
    "    #Ignore empty articles\n",
    "    if isinstance(row[1]['cleaned_text'], str):\n",
    "        docs.append(document_tup(row[1]['cleaned_text'].split(), \n",
    "                                 [row[1]['name']]\n",
    "                                 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1455b9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to build the vocabulary based on 5954 documents...\n",
      "Starting to train the model for 5 epochs and with vector size 64...\n",
      "Finished. Total time to train: 0.31995852788289386 min...\n"
     ]
    }
   ],
   "source": [
    "def doc2vec(docs:namedtuple, vector_size, window, \n",
    "            min_count, workers, epochs):\n",
    "    \"\"\"\n",
    "    Fits the document doc2vec model on a list of namedtuples.\n",
    "    Returns the trained model.\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    model = Doc2Vec(vector_size=vector_size, window=window, min_count=min_count, workers=workers, epochs=epochs)\n",
    "    print(f'Starting to build the vocabulary based on {len(docs)} documents...')\n",
    "    model.build_vocab(docs)\n",
    "    print(f'Starting to train the model for {epochs} epochs and with vector size {vector_size}...')\n",
    "    model.train(docs, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "    print(f'Finished. Total time to train: {(time.time()-start_time) / 60} min...')\n",
    "    return model\n",
    "\n",
    "model = doc2vec(docs, vector_size=64, window=20, min_count=10, workers=8, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b349b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extract the document embeddings from trained model\n",
    "doc_labs = list(model.dv.key_to_index.keys())\n",
    "doc_embs = np.array([model.dv[lab] for lab in doc_labs])\n",
    "doc_df = pd.DataFrame(doc_embs, index = doc_labs)\n",
    "#Save the embeddings and labels locally as TSV\n",
    "with open('document_meta.tsv','w+', encoding='utf-8') as file_metadata:\n",
    "    for lab in doc_labs:\n",
    "        file_metadata.write(lab+'\\n')\n",
    "\n",
    "doc_df.to_csv('document_embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b545aa",
   "metadata": {},
   "source": [
    "## Node Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edbeca4",
   "metadata": {},
   "source": [
    "`Node2vec` was introduced in {citep}`grover2016node2vec` and is in many ways just like `doc2vec` explained above with the noteable difference that we are working with a graph instead of document of text. The trick of `node2vec` is to first create a representation of the graph as a string that encodes the connection between nodes. As visualized bellow, this is done by taking random walks in the graph and letting the connections form artificial \"sentences\". This leads to a data structure that can be passed to the normal `word2vec` model and generates one embedding corresponding to each node. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1971d838",
   "metadata": {},
   "source": [
    "![node](https://miro.medium.com/max/1838/1*GbZk_M_HqCu8Y99J_FzhQw.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc15494",
   "metadata": {},
   "source": [
    "Before running the model we load the edge list and create the undirected `networkx` graph object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7eeaffb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "edgelist = pd.read_pickle(\"https://drive.google.com/uc?export=download&id=1x1WOVm5Wp6SLfN1sePSdgorbAaGQSYR3\")\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a502046d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4186/4186 [00:09<00:00, 420.27it/s]\n",
      "Generating walks (CPU: 1): 100%|██████████| 25/25 [01:20<00:00,  3.22s/it]\n",
      "Generating walks (CPU: 2): 100%|██████████| 25/25 [01:22<00:00,  3.31s/it]\n",
      "Generating walks (CPU: 4): 100%|██████████| 25/25 [01:29<00:00,  3.60s/it]\n",
      "Generating walks (CPU: 6): 100%|██████████| 25/25 [01:43<00:00,  4.14s/it]\n",
      "Generating walks (CPU: 8): 100%|██████████| 25/25 [01:40<00:00,  4.04s/it]\n",
      "Generating walks (CPU: 3): 100%|██████████| 25/25 [01:54<00:00,  4.59s/it]\n",
      "Generating walks (CPU: 5): 100%|██████████| 25/25 [01:54<00:00,  4.59s/it]\n",
      "Generating walks (CPU: 7): 100%|██████████| 25/25 [01:55<00:00,  4.62s/it]\n"
     ]
    }
   ],
   "source": [
    "#Train the model\n",
    "model = Node2Vec(G, dimensions=64, walk_length=40, num_walks=200, workers=8).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7a32a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_labs = list(fit.wv.key_to_index.keys())\n",
    "node_embs = [fit.wv[lab] for lab in node_labs]\n",
    "node_df = pd.DataFrame(node_embs, index = node_labs)\n",
    "#Save the embeddings and labels locally as TSV\n",
    "with open('node_meta.tsv','w+', encoding='utf-8') as file_metadata:\n",
    "    for lab in doc_labs:\n",
    "        file_metadata.write(lab+'\\n')\n",
    "\n",
    "node_df.to_csv('node_embeddings.tsv', sep='\\t', index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "794ed8d0b773cbcd7b7ec7f3b705e2b7a8437127647cc51e7d6c4b4e7ca1b777"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('pyenv': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
