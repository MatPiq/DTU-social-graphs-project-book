Traceback (most recent call last):
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/jupyter_cache/executors/utils.py", line 56, in single_nb_execution
    record_timing=False,
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/client.py", line 1093, in execute
    return NotebookClient(nb=nb, resources=resources, km=km, **kwargs).execute()
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/util.py", line 84, in wrapped
    return just_run(coro(*args, **kwargs))
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/util.py", line 62, in just_run
    return loop.run_until_complete(coro)
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/asyncio/base_events.py", line 587, in run_until_complete
    return future.result()
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/client.py", line 560, in async_execute
    cell, index, execution_count=self.code_cells_executed + 1
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/client.py", line 854, in async_execute_cell
    self._check_raise_for_error(cell, exec_reply)
  File "/opt/hostedtoolcache/Python/3.7.12/x64/lib/python3.7/site-packages/nbclient/client.py", line 756, in _check_raise_for_error
    raise CellExecutionError.from_cell_and_msg(cell, exec_reply_content)
nbclient.exceptions.CellExecutionError: An error occurred while executing the following cell:
------------------
def collect_attributes(articles:list[WikiPage])->list[WikiPage]:
    """
    Parses the wikipedia article text and urls pointing to another wiki page.
    """
    base_url = 'https://en.wikipedia.org/wiki/'
    error_log = dict()
    for page in tqdm(pages):
        try:
            try: 
                resp = requests.get(base_url+page.title, timeout=10)
            except requests.exceptions.Timeout as e: 
                error_log[page.title] = e
            
            soup = BeautifulSoup(resp.content, 'html.parser')
            content = soup.find('div', {'id':'mw-content-text'})
            text = ''
            for paragraph in content.find_all('p'):
                text += ' ' + paragraph.text
            page.text = text
            page.edges = [ref.text for ref in content.find_all('a', href=True) 
                                               if 'wiki' in ref.get('href')]
        except Exception as e:
            #Log potential errors in collection
            error_log[page.title] = str(e)
            
    return pages, error_log

pages, error_log = collect_attributes(pages)
------------------

[0;31m---------------------------------------------------------------------------[0m
[0;31mTypeError[0m                                 Traceback (most recent call last)
[0;32m/tmp/ipykernel_1651/749677908.py[0m in [0;36m<module>[0;34m[0m
[0;32m----> 1[0;31m [0;32mdef[0m [0mcollect_attributes[0m[0;34m([0m[0marticles[0m[0;34m:[0m[0mlist[0m[0;34m[[0m[0mWikiPage[0m[0;34m][0m[0;34m)[0m[0;34m->[0m[0mlist[0m[0;34m[[0m[0mWikiPage[0m[0;34m][0m[0;34m:[0m[0;34m[0m[0;34m[0m[0m
[0m[1;32m      2[0m     """
[1;32m      3[0m     [0mParses[0m [0mthe[0m [0mwikipedia[0m [0marticle[0m [0mtext[0m [0;32mand[0m [0murls[0m [0mpointing[0m [0mto[0m [0manother[0m [0mwiki[0m [0mpage[0m[0;34m.[0m[0;34m[0m[0;34m[0m[0m
[1;32m      4[0m     """
[1;32m      5[0m     [0mbase_url[0m [0;34m=[0m [0;34m'https://en.wikipedia.org/wiki/'[0m[0;34m[0m[0;34m[0m[0m

[0;31mTypeError[0m: 'type' object is not subscriptable
TypeError: 'type' object is not subscriptable

