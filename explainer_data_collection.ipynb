{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines our data collection strategy which consists of the following steps:\n",
    "\n",
    "1. Finding the relevant Wikipedia pages for each discipline through [PetScan](https://petscan.wmflabs.org/).\n",
    "2. Scraping each page to parse out hyperlinks to other Wikipedia pages and the text.\n",
    "3. Creating a smaller and manageble subgraph from the Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T12:33:52.369894Z",
     "start_time": "2021-12-01T12:33:46.884437Z"
    }
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests\n",
    "import networkx as nx\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from itertools import chain\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T12:33:52.562523Z",
     "start_time": "2021-12-01T12:33:52.388388Z"
    }
   },
   "outputs": [],
   "source": [
    "import littleballoffur"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding relevant articles\n",
    "\n",
    "To collect the relevant wikipedia pages for our project we specify the dataclass `WikiPage`. This is based on the use of the open-source software [PetScan](https://petscan.wmflabs.org/) that based on a list of wikipedia-categories yields the corresponding page-names. We furthermore specify the depth of our PetScan-query, which is a measure of how deep we want our categories to be. As the list of pages grows exponentially we limit the levels of depth we set the parameter to 0, 1 and 2. The reason for not choosing one specific depth is that the group and sub-group structure of the disciplines differs which means that we get a widely different amount of pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=False)\n",
    "class WikiPage:\n",
    "    \"\"\"\n",
    "    Data obj that stores an article and \n",
    "    its relevant attributes\n",
    "    \"\"\"\n",
    "    title:str\n",
    "    parent:str\n",
    "    depth:int\n",
    "    text:str = np.nan\n",
    "    edges:List = np.nan\n",
    "        \n",
    "\n",
    "def collect_pages(parents:list,\n",
    "                  depth:int=0)->List[WikiPage]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds relevant articles from petscan based on some initial query.\n",
    "    See https://petscan.wmflabs.org/ for api reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages = list()\n",
    "    errors = 0\n",
    "    #setup API call\n",
    "    base_url = 'https://petscan.wmflabs.org/?ns%5B0%5D=1&'\n",
    "    params = {'project':'wikipedia',\n",
    "              'language':'en',\n",
    "              'format':'json',\n",
    "              'interface_language':'en',\n",
    "              'depth':str(depth),\n",
    "              'doit':''}\n",
    "    \n",
    "    #Loop over parents and get corresponding page names\n",
    "    for cat in parents:\n",
    "        params['categories'] = cat\n",
    "        resp = requests.get(url=base_url, params=params).json()\n",
    "        try: \n",
    "            for page in resp['*'][0]['a']['*']:\n",
    "\n",
    "                #Append nodes\n",
    "                pages.append(WikiPage(title=page['title'],\n",
    "                                      parent=cat,\n",
    "                                      depth=depth))\n",
    "                \n",
    "        except KeyError:\n",
    "            errors+=1\n",
    "    \n",
    "    print(f'Petscan failed to retrieve {errors} pages in depth {depth}...')\n",
    "            \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we call the function `collect_pages` and create a page list for depth 0, 1 and 2 and display the resulting counts. As can be seen Anthropology is a clear outlier because of a different group structure on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37ac94ffd394c4ea62e83ab5770dfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petscan failed to retrieve 0 pages in depth 0...\n",
      "Petscan failed to retrieve 0 pages in depth 1...\n",
      "Petscan failed to retrieve 0 pages in depth 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parent\n",
       "anthropology         17621\n",
       "economics             6023\n",
       "political_science     7011\n",
       "psychology            8757\n",
       "sociology             5895\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define initial query groups\n",
    "query = ['political_science', 'economics', \n",
    "          'sociology', 'anthropology', \n",
    "          'psychology']\n",
    "\n",
    "depths = [0,1,2]\n",
    "pages = []\n",
    "for d in tqdm(depths):\n",
    "    pages += collect_pages(query, d)\n",
    "#Show marginal distribution    \n",
    "pd.DataFrame(pages).groupby('parent').count()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikiPage(title='Talking_shit', parent='sociology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Fan_effect', parent='psychology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Ernst-Ludwig_von_Thadden', parent='economics', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Hassan_Kettani', parent='political_science', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Ethel_Cutler_Freeman', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Theory_of_generations', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='National_Pet_Month', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Janine_Krieber', parent='political_science', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Peter_Lewis_Paul', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Toxic_masculinity', parent='sociology', depth=2, text=nan, edges=nan)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display some random articles\n",
    "random.sample(pages, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect page text and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `collect_attributes` we use `BeautifulSoup` to scrape the html content from the wikipedia pages we've found. The key html node is the `div` with attributes `{'id':'mw-content-text'}` from which we can parse out all paragraphs and hyperlinks, disregarding section headings, tables and other irrelevant content and page attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696250c0808944f5988c82a5f1b5e234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collect_attributes(articles:list[WikiPage])->list[WikiPage]:\n",
    "    \"\"\"\n",
    "    Parses the wikipedia article text and urls pointing to another wiki page.\n",
    "    \"\"\"\n",
    "    base_url = 'https://en.wikipedia.org/wiki/'\n",
    "    error_log = dict()\n",
    "    for page in tqdm(pages):\n",
    "        try:\n",
    "            try: \n",
    "                resp = requests.get(base_url+page.title, timeout=10)\n",
    "            except requests.exceptions.Timeout as e: \n",
    "                error_log[page.title] = e\n",
    "            \n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            content = soup.find('div', {'id':'mw-content-text'})\n",
    "            text = ''\n",
    "            for paragraph in content.find_all('p'):\n",
    "                text += ' ' + paragraph.text\n",
    "            page.text = text\n",
    "            page.edges = [ref.text for ref in content.find_all('a', href=True) \n",
    "                                               if 'wiki' in ref.get('href')]\n",
    "        except Exception as e:\n",
    "            #Log potential errors in collection\n",
    "            error_log[page.title] = str(e)\n",
    "            \n",
    "    return pages, error_log\n",
    "\n",
    "pages, error_log = collect_attributes(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of pages that failed to be collected: 135\n"
     ]
    }
   ],
   "source": [
    "print(f'Amount of pages that failed to be collected: {len(error_log.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting a smaler network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the large size of the network, we deem it necessary to create a smaller subgraph that is more manageble.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 704,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:46.189910Z",
     "start_time": "2021-12-01T21:34:14.606540Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_df = pd.read_pickle(\"full_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 705,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:46.203139Z",
     "start_time": "2021-12-01T21:34:46.196699Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_anthro(df):\n",
    "    df = df.loc[~((df[\"depth\"] == 2) & (df[\"parent\"] == \"anthropology\"))]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:46.306914Z",
     "start_time": "2021-12-01T21:34:46.211629Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth  text  edges\n",
       "parent                                      \n",
       "anthropology        2161   2161  2161   2161\n",
       "economics           6023   6023  6023   6023\n",
       "political_science   7011   7011  7011   7011\n",
       "psychology          8757   8757  8757   8757\n",
       "sociology           5895   5895  5895   5895"
      ]
     },
     "execution_count": 706,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_df.loc[~((pages_df[\"depth\"] == 2) & (pages_df[\"parent\"] == \"anthropology\"))].groupby(\"parent\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 697,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:32:27.386362Z",
     "start_time": "2021-12-01T21:32:27.323045Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>17621</td>\n",
       "      <td>17621</td>\n",
       "      <td>17621</td>\n",
       "      <td>17621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth   text  edges\n",
       "parent                                       \n",
       "anthropology       17621  17621  17621  17621\n",
       "economics           6023   6023   6023   6023\n",
       "political_science   7011   7011   7011   7011\n",
       "psychology          8757   8757   8757   8757\n",
       "sociology           5895   5895   5895   5895"
      ]
     },
     "execution_count": 697,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pages_df.groupby(\"parent\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:46.352652Z",
     "start_time": "2021-12-01T21:34:46.310638Z"
    }
   },
   "outputs": [],
   "source": [
    "df = remove_anthro(pages_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:46.449108Z",
     "start_time": "2021-12-01T21:34:46.365999Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "      <td>2161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "      <td>6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "      <td>7011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "      <td>8757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "      <td>5895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth  text  edges\n",
       "parent                                      \n",
       "anthropology        2161   2161  2161   2161\n",
       "economics           6023   6023  6023   6023\n",
       "political_science   7011   7011  7011   7011\n",
       "psychology          8757   8757  8757   8757\n",
       "sociology           5895   5895  5895   5895"
      ]
     },
     "execution_count": 708,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"parent\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:34:48.508834Z",
     "start_time": "2021-12-01T21:34:48.488104Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_duplicates(df):\n",
    "    nodes_to_remove = [node for node in tqdm(set(df[df.duplicated(\"title\")][\"title\"])) if\n",
    "                      len(set(df[df[\"title\"] == node][\"parent\"])) > 1]\n",
    "    \n",
    "    df = df[~df['title'].isin(nodes_to_remove)]\n",
    "    df = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:35:13.937643Z",
     "start_time": "2021-12-01T21:34:50.238866Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 5714/5714 [00:23<00:00, 241.87it/s]\n"
     ]
    }
   ],
   "source": [
    "df = remove_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:35:14.006563Z",
     "start_time": "2021-12-01T21:35:13.943449Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth  text  edges\n",
       "parent                                      \n",
       "anthropology        1401   1401  1401   1401\n",
       "economics           4636   4636  4636   4636\n",
       "political_science   5217   5217  5217   5217\n",
       "psychology          6715   6715  6715   6715\n",
       "sociology           3518   3518  3518   3518"
      ]
     },
     "execution_count": 711,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"parent\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:35:36.990927Z",
     "start_time": "2021-12-01T21:35:36.976533Z"
    }
   },
   "outputs": [],
   "source": [
    "def uniform_page_and_edge_names(df):\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    df['edges'] = df['edges'].apply(lambda x: [re.sub(' ', '_', l.strip().lower()) for l in x])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:35:38.725253Z",
     "start_time": "2021-12-01T21:35:37.770490Z"
    }
   },
   "outputs": [],
   "source": [
    "df = uniform_page_and_edge_names(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:35:47.343723Z",
     "start_time": "2021-12-01T21:35:47.332162Z"
    }
   },
   "outputs": [],
   "source": [
    "def remove_edges_not_in_nodelist(df):\n",
    "    tqdm.pandas()\n",
    "    nodes = df[\"title\"].tolist()\n",
    "    df['edges'] = df['edges'].progress_apply(lambda x: [e for e in x if e in nodes])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:40:03.589047Z",
     "start_time": "2021-12-01T21:35:52.902032Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 21487/21487 [04:10<00:00, 85.72it/s]\n"
     ]
    }
   ],
   "source": [
    "df = remove_edges_not_in_nodelist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:40:54.969219Z",
     "start_time": "2021-12-01T21:40:54.935339Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "      <td>1401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "      <td>4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "      <td>5217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "      <td>6715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "      <td>3518</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth  text  edges\n",
       "parent                                      \n",
       "anthropology        1401   1401  1401   1401\n",
       "economics           4636   4636  4636   4636\n",
       "political_science   5217   5217  5217   5217\n",
       "psychology          6715   6715  6715   6715\n",
       "sociology           3518   3518  3518   3518"
      ]
     },
     "execution_count": 716,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"parent\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T17:40:27.376940Z",
     "start_time": "2021-12-01T17:40:27.351993Z"
    }
   },
   "outputs": [],
   "source": [
    "#def get_edgelist(df):\n",
    "#    edgelist = df.apply(lambda x: [(x.title, edge) for edge in x.edges], axis = 1)\n",
    "#    edgelist = chain.from_iterable(edgelist)\n",
    "#    return list(edgelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T17:40:27.901843Z",
     "start_time": "2021-12-01T17:40:27.893279Z"
    }
   },
   "outputs": [],
   "source": [
    "#def remove_self_loops(df, edgelist):\n",
    "#    edgelist = [e for e in edgelist if e[0] != e[1]]\n",
    "    #list_of_nodes_to_keep = [e[0] for e in edgelist] + [e[1] for e in edgelist]\n",
    "    #list_of_nodes_to_keep = list(set(list_of_nodes_to_keep))\n",
    "    #df = df[df['title'].isin(list_of_nodes_to_keep)]\n",
    "#    return edgelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T19:31:11.133038Z",
     "start_time": "2021-12-01T19:29:29.309372Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 8004/8004 [00:43<00:00, 183.70it/s]\n",
      "100%|████████████████████████████████████| 15226/15226 [00:57<00:00, 263.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df = remove_duplicates(pages_df)\n",
    "df = remove_anthro(df)\n",
    "df = uniform_page_and_edge_names(df)\n",
    "df = remove_edges_not_in_nodelist(df)\n",
    "#edgelist = get_edgelist(df)\n",
    "#df = remove_self_loops(df, edgelist)\n",
    "#df = extract_connected_nodes(df, edgelist)\n",
    "#df = remove_edges_not_in_nodelist(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:41:03.053836Z",
     "start_time": "2021-12-01T21:41:03.042549Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:41:51.550852Z",
     "start_time": "2021-12-01T21:41:48.254771Z"
    }
   },
   "outputs": [],
   "source": [
    "node_attr = df[[\"title\", \"parent\", \"depth\"]].to_dict(\"index\")\n",
    "index_dict = {i:k for k, i in enumerate(df['title'])}\n",
    "\n",
    "edge_list = []\n",
    "for node, edges in zip(df['title'].tolist(), df['edges'].tolist()):\n",
    "    for edge in edges:\n",
    "        edge_list.append((index_dict[node], index_dict[edge]))\n",
    "edge_list = [e for e in edge_list if e[0] != e[1]]\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(list(index_dict.values()))\n",
    "nx.set_node_attributes(G, node_attr)\n",
    "G.add_edges_from(edge_list)\n",
    "gcc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(gcc)\n",
    "\n",
    "G = nx.relabel.convert_node_labels_to_integers(G)\n",
    "from littleballoffur import MetropolisHastingsRandomWalkSampler\n",
    "\n",
    "number_of_nodes = int(0.25 * G.number_of_nodes())\n",
    "sampler = MetropolisHastingsRandomWalkSampler(number_of_nodes = number_of_nodes)\n",
    "new_graph = sampler.sample(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:41:51.660151Z",
     "start_time": "2021-12-01T21:41:51.593067Z"
    }
   },
   "outputs": [],
   "source": [
    "parent=nx.get_node_attributes(new_graph,'parent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:41:51.706331Z",
     "start_time": "2021-12-01T21:41:51.689290Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'political_science': 1092,\n",
       "         'psychology': 1587,\n",
       "         'economics': 985,\n",
       "         'anthropology': 338,\n",
       "         'sociology': 808})"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(parent.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:54:10.100926Z",
     "start_time": "2021-12-01T21:54:07.783830Z"
    }
   },
   "outputs": [],
   "source": [
    "nodes_to_keep = [list(index_dict.keys())[i] for i in list(new_graph.nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 748,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T21:57:15.243831Z",
     "start_time": "2021-12-01T21:57:15.218371Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df = df[df['title'].isin(nodes_to_keep)].reset_index()[[\"title\", \"parent\", \"depth\", \"text\", \"edges\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 752,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:00:28.924725Z",
     "start_time": "2021-12-01T22:00:22.172728Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 4810/4810 [00:06<00:00, 717.81it/s]\n"
     ]
    }
   ],
   "source": [
    "final_df = remove_edges_not_in_nodelist(final_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 794,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:08:39.448851Z",
     "start_time": "2021-12-01T22:08:39.426350Z"
    }
   },
   "outputs": [],
   "source": [
    "node_attr = final_df[[\"title\", \"parent\", \"depth\"]].set_index(\"title\").to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:12:12.506213Z",
     "start_time": "2021-12-01T22:12:12.414978Z"
    }
   },
   "outputs": [],
   "source": [
    "edge_list = []\n",
    "for node, edges in zip(final_df['title'].tolist(), final_df['edges'].tolist()):\n",
    "    for edge in edges:\n",
    "        edge_list.append((node, edge))\n",
    "\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edge_list)\n",
    "gcc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(gcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:12:13.309670Z",
     "start_time": "2021-12-01T22:12:13.290307Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df[\"gcc\"] = final_df[\"title\"].apply(lambda x: 1 if x in G.nodes() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:12:56.325515Z",
     "start_time": "2021-12-01T22:12:56.224298Z"
    }
   },
   "outputs": [],
   "source": [
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edge_list)\n",
    "node_attr = final_df[[\"title\", \"parent\", \"depth\", \"gcc\"]].set_index(\"title\").to_dict(\"index\")\n",
    "nx.set_node_attributes(G, node_attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:14:20.229778Z",
     "start_time": "2021-12-01T22:14:20.223935Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 829,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:14:25.251450Z",
     "start_time": "2021-12-01T22:14:25.184252Z"
    }
   },
   "outputs": [],
   "source": [
    "pickle.dump(G, open('Final_graph.pickle', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-01T22:14:48.214900Z",
     "start_time": "2021-12-01T22:14:43.491659Z"
    }
   },
   "outputs": [],
   "source": [
    "final_df.to_pickle(\"Final_df.pickle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the collection of pages we gather them in a dataframe and edgelist for future use. To reduce the size of edgelist we alreay now remove edges that points to pages we have not collected. This means that we only keep edges that link to other pages in one of the five categories."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
