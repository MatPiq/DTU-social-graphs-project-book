{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines our data collection strategy which consists of the following steps:\n",
    "\n",
    "1. Finding the relevant Wikipedia pages for each discipline through [PetScan](https://petscan.wmflabs.org/).\n",
    "2. Scraping each page to parse out hyperlinks to other Wikipedia pages and the text.\n",
    "3. Creating a smaller and manageble subgraph from the Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import requests\n",
    "import json\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random \n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding relevant articles\n",
    "\n",
    "To collect the relevant wikipedia pages for our project we specify the dataclass `WikiPage`. This is based on the use of the open-source software [PetScan](https://petscan.wmflabs.org/) that based on a list of wikipedia-categories yields the corresponding page-names. We furthermore specify the depth of our PetScan-query, which is a measure of how deep we want our categories to be. As the list of pages grows exponentially we limit the levels of depth we set the parameter to 0, 1 and 2. The reason for not choosing one specific depth is that the group and sub-group structure of the disciplines differs which means that we get a widely different amount of pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=False)\n",
    "class WikiPage:\n",
    "    \"\"\"\n",
    "    Data obj that stores an article and \n",
    "    its relevant attributes\n",
    "    \"\"\"\n",
    "    title:str\n",
    "    parent:str\n",
    "    depth:int\n",
    "    text:str = np.nan\n",
    "    edges:List = np.nan\n",
    "        \n",
    "\n",
    "def collect_pages(parents:list,\n",
    "                  depth:int=0)->List[WikiPage]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds relevant articles from petscan based on some initial query.\n",
    "    See https://petscan.wmflabs.org/ for api reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages = list()\n",
    "    errors = 0\n",
    "    #setup API call\n",
    "    base_url = 'https://petscan.wmflabs.org/?ns%5B0%5D=1&'\n",
    "    params = {'project':'wikipedia',\n",
    "              'language':'en',\n",
    "              'format':'json',\n",
    "              'interface_language':'en',\n",
    "              'depth':str(depth),\n",
    "              'doit':''}\n",
    "    \n",
    "    #Loop over parents and get corresponding page names\n",
    "    for cat in parents:\n",
    "        params['categories'] = cat\n",
    "        resp = requests.get(url=base_url, params=params).json()\n",
    "        try: \n",
    "            for page in resp['*'][0]['a']['*']:\n",
    "\n",
    "                #Append nodes\n",
    "                pages.append(WikiPage(title=page['title'],\n",
    "                                      parent=cat,\n",
    "                                      depth=depth))\n",
    "                \n",
    "        except KeyError:\n",
    "            errors+=1\n",
    "    \n",
    "    print(f'Petscan failed to retrieve {errors} pages in depth {depth}...')\n",
    "            \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we call the function `collect_pages` and create a page list for depth 0, 1 and 2 and display the resulting counts. As can be seen Anthropology is a clear outlier because of a different group structure on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05f6a20c9b2c4449b106b6bf8a449145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petscan failed to retrieve 0 pages in depth 0...\n",
      "Petscan failed to retrieve 0 pages in depth 1...\n",
      "Petscan failed to retrieve 0 pages in depth 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parent\n",
       "anthropology         17621\n",
       "economics             6023\n",
       "political_science     7011\n",
       "psychology            8785\n",
       "sociology             5895\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define initial query groups\n",
    "query = ['political_science', 'economics', \n",
    "          'sociology', 'anthropology', \n",
    "          'psychology']\n",
    "\n",
    "depths = [0,1,2]\n",
    "pages = []\n",
    "for d in tqdm(depths):\n",
    "    pages += collect_pages(query, d)\n",
    "#Show marginal distribution    \n",
    "pd.DataFrame(pages).groupby('parent').count()['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikiPage(title='Adolescent_crystallization', parent='psychology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title=\"Maimonides'_rule\", parent='psychology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Affine_pricing', parent='economics', depth=1, text=nan, edges=nan),\n",
       " WikiPage(title='Orgasmic_platform', parent='sociology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Box_office_futures', parent='economics', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Laurent_Naud', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Psychiatric_casualty', parent='psychology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Western_painting', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='Hunnic_language', parent='anthropology', depth=2, text=nan, edges=nan),\n",
       " WikiPage(title='The_Culture_of_Connectivity', parent='political_science', depth=2, text=nan, edges=nan)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Display some random articles\n",
    "random.sample(pages, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect page text and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `collect_attributes` we use `BeautifulSoup` to scrape the html content from the wikipedia pages we've found. The key html node is the `div` with attributes `{'id':'mw-content-text'}` from which we can parse out all paragraphs and hyperlinks, disregarding section headings, tables and other irrelevant content and page attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_attributes(articles:list[WikiArticle])->list[WikiArticle]:\n",
    "    \"\"\"\n",
    "    Parses the wikipedia article text and urls pointing to another wiki page.\n",
    "    \"\"\"\n",
    "\n",
    "    base_url = 'https://en.wikipedia.org/wiki/'\n",
    "    for page in tqdm(pages):\n",
    "        resp = requests.get(base_url+page.name)\n",
    "        soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "        content = soup.find('div', {'id':'mw-content-text'})\n",
    "        text = ''\n",
    "        for paragraph in content.find_all('p'):\n",
    "            text += ' ' + paragraph.text\n",
    "        node.text = text\n",
    "        node.edges = [ref.text for ref in content.find_all('a', href=True) \n",
    "                                             if 'wiki' in ref.get('href')]\n",
    "    return pages\n",
    "\n",
    "collect_attributes(pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting a smaler network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the large size of the network, we deem it necessary to create a smaller subgraph that is more manageble.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the collection of pages we gather them in a dataframe and edgelist for future use. To reduce the size of edgelist we alreay now remove edges that points to pages we have not collected. This means that we only keep edges that link to other pages in one of the five categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our nodes we can now create and save our df for future use\n",
    "def create_df(nodes = nodes):\n",
    "    return pd.DataFrame({\"name\": [node.name for node in nodes],\n",
    "                         \"parent\": [node.parent for node in nodes],\n",
    "                         \"depth\": [node.depth for node in nodes],\n",
    "                         \"edges\": [node.edges for node in nodes],\n",
    "                         \"text\": [node.text for node in nodes],\n",
    "                         \"categories\": [node.categories for node in nodes]})\n",
    "\n",
    "df = create_df()\n",
    "df.to_pickle(\"df.obj\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on our nodes we can now create a edgelist and save our ot for future use\n",
    "def create_edgelist(nodes = nodes):\n",
    "    nodelist = [node.name for node in nodes]\n",
    "    edgelist = [[(nodes[i].name, edge) for edge in nodes[i].edges if edge in nodelist]\n",
    "                for i in tqdm(range(len(nodes)))]\n",
    "    return list(chain.from_iterable(edgelist))\n",
    "\n",
    "edgelist = create_edgelist()\n",
    "with open('edgelist.obj', 'wb') as f:\n",
    "    pickle.dump(edgelist, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
