{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(explainer_data_collection)=\n",
    "Data collection\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outlines our data collection strategy which consists of the following steps:\n",
    "\n",
    "1. Finding the relevant Wikipedia pages for each discipline through [PetScan](https://petscan.wmflabs.org/).\n",
    "2. Scraping each page to parse out hyperlinks to other Wikipedia pages and the text.\n",
    "3. Creating a smaller and manageble subgraph from the Network.\n",
    "\n",
    "Lastly we will also describe the steps that we have taken to preprocess our data for the purphose of later natural language processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-04T13:41:49.184172Z",
     "start_time": "2021-12-04T13:41:43.772868Z"
    }
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import random \n",
    "import requests\n",
    "import pickle\n",
    "import warnings\n",
    "import powerlaw\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from typing import List\n",
    "from itertools import chain\n",
    "from bs4 import BeautifulSoup\n",
    "from tqdm.notebook import tqdm\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import networkx as nx\n",
    "import littleballoffur\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding relevant articles\n",
    "\n",
    "To collect the relevant wikipedia pages for our project we specify the dataclass `WikiPage`. This is based on the use of the open-source software [PetScan](https://petscan.wmflabs.org/) that based on a list of wikipedia-categories yields the corresponding page-names. We furthermore specify the depth of our PetScan-query, which is a measure of how deeply nested we want our categories to be. As the list of pages grows exponentially we limit the levels of depth we set the parameter to 0, 1 and 2. The reason for not choosing one specific depth is that the group and sub-group structure of the disciplines differs which means that we get a widely different amount of pages. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=False)\n",
    "class WikiPage:\n",
    "    \"\"\"\n",
    "    Data obj that stores an article and \n",
    "    its relevant attributes\n",
    "    \"\"\"\n",
    "    title:str\n",
    "    parent:str\n",
    "    depth:int\n",
    "    text:str = np.nan\n",
    "    edges:List = np.nan\n",
    "        \n",
    "\n",
    "def collect_pages(parents:list,\n",
    "                  depth:int=0)->List[WikiPage]:\n",
    "    \n",
    "    \"\"\"\n",
    "    Finds relevant articles from petscan based on some initial query.\n",
    "    See https://petscan.wmflabs.org/ for api reference.\n",
    "    \"\"\"\n",
    "    \n",
    "    pages = list()\n",
    "    errors = 0\n",
    "    #setup API call\n",
    "    base_url = 'https://petscan.wmflabs.org/?ns%5B0%5D=1&'\n",
    "    params = {'project':'wikipedia',\n",
    "              'language':'en',\n",
    "              'format':'json',\n",
    "              'interface_language':'en',\n",
    "              'depth':str(depth),\n",
    "              'doit':''}\n",
    "    \n",
    "    #Loop over parents and get corresponding page names\n",
    "    for cat in parents:\n",
    "        params['categories'] = cat\n",
    "        resp = requests.get(url=base_url, params=params).json()\n",
    "        try: \n",
    "            for page in resp['*'][0]['a']['*']:\n",
    "\n",
    "                #Append nodes\n",
    "                pages.append(WikiPage(title=page['title'],\n",
    "                                      parent=cat,\n",
    "                                      depth=depth))\n",
    "                \n",
    "        except KeyError:\n",
    "            errors+=1\n",
    "    \n",
    "    print(f'Petscan failed to retrieve {errors} pages in depth {depth}...')\n",
    "    \n",
    "    return pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bellow we call the function `collect_pages` and create a page list for depth 0, 1 and 2 and display the resulting counts. As can be seen Anthropology is a clear outlier because of a different group structure on wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d37ac94ffd394c4ea62e83ab5770dfab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Petscan failed to retrieve 0 pages in depth 0...\n",
      "Petscan failed to retrieve 0 pages in depth 1...\n",
      "Petscan failed to retrieve 0 pages in depth 2...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "parent\n",
       "anthropology         17621\n",
       "economics             6023\n",
       "political_science     7011\n",
       "psychology            8757\n",
       "sociology             5895\n",
       "Name: title, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define initial query groups\n",
    "query = ['political_science', 'economics', 'sociology', 'anthropology', 'psychology']\n",
    "\n",
    "depths = [0,1,2]\n",
    "pages = []\n",
    "for d in tqdm(depths):\n",
    "    pages += collect_pages(query, d)\n",
    "    \n",
    "#Show marginal distribution of pages\n",
    "pd.DataFrame(pages).groupby('parent').count()['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect page text and edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the function `collect_attributes` we use `BeautifulSoup` to scrape the html content from the wikipedia pages we've found. The key html node is the `div` with attributes `{'id':'mw-content-text'}` from which we can parse out all paragraphs and hyperlinks, disregarding section headings, tables and other irrelevant content and page attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696250c0808944f5988c82a5f1b5e234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45307 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def collect_attributes(articles:list[WikiPage])->list[WikiPage]:\n",
    "    \"\"\"\n",
    "    Parses the wikipedia article text and urls pointing to another wiki page.\n",
    "    \"\"\"\n",
    "    base_url = 'https://en.wikipedia.org/wiki/'\n",
    "    error_log = dict()\n",
    "    for page in tqdm(pages):\n",
    "        try:\n",
    "            try: \n",
    "                resp = requests.get(base_url+page.title, timeout=10)\n",
    "            except requests.exceptions.Timeout as e: \n",
    "                error_log[page.title] = e\n",
    "            \n",
    "            soup = BeautifulSoup(resp.content, 'html.parser')\n",
    "            content = soup.find('div', {'id':'mw-content-text'})\n",
    "            text = ''\n",
    "            for paragraph in content.find_all('p'):\n",
    "                text += ' ' + paragraph.text\n",
    "            page.text = text\n",
    "            page.edges = [ref.text for ref in content.find_all('a', href=True) \n",
    "                                               if 'wiki' in ref.get('href')]\n",
    "        except Exception as e:\n",
    "            #Log potential errors in collection\n",
    "            error_log[page.title] = str(e)\n",
    "            \n",
    "    return pages, error_log\n",
    "\n",
    "pages, error_log = collect_attributes(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages_df = pd.DataFrame(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amount of pages that failed to be collected: 135\n"
     ]
    }
   ],
   "source": [
    "print(f'Amount of pages that failed to be collected: {len(error_log.keys())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As only 135 out of 45307 pages are missing we do not consider this a problem of substantiel value, why we keep out dataset the way it is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting a smaler network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because of the large size of the network, we deem it necessary to create a smaller subgraph that is more manageble. To do this we initially perform three operations:\n",
    "\n",
    "- (1) **Restrict anthropology pages to depth = 1**: \n",
    "Due to the inherent strcuture of Wikipedia we can not expect all of the categories to have an equal amount of pages. \n",
    "Some categories might be defined more loosely and therefore span broader, whereas other categories might be larger as more people are participates in activities related to the given discpline and therefore more prone to write a Wikipedia page about. No matter the reason, anthropology pages are widely overrepresentative in our dataset, why we decide to restrict the anthropology pages to `depth = 1`.\n",
    "- (2) **Remove Duplicates**: \n",
    "Some pages occur in two categories e.g. the page *elitism* occur both in political science and anthropology category. As it becomes ambigious to which discipline the page belong we decide to remove them. In cases where some pages occurs several times within the same category (as they are collected at different depths) we collapse them to one observation.\n",
    "- (3) **Remove edges to pages that did not collect**: \n",
    "Lastly we remove all edges from the pages we have collected to pages not in our sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T09:51:54.509202Z",
     "start_time": "2021-12-05T09:50:38.096700Z"
    }
   },
   "outputs": [],
   "source": [
    "pages_df = pd.read_pickle(\"full_data.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:24:03.683261Z",
     "start_time": "2021-12-05T10:09:38.584337Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb827056769a4e83bb885986a414ef44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8004 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f2b1be6ad7649abbd8f03e04bf81b27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20061 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def remove_anthro(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove pages in the anthropology category of depth 2\n",
    "    \"\"\"\n",
    "    df = df.loc[~((df[\"depth\"] == 2) & (df[\"parent\"] == \"anthropology\"))]\n",
    "    return df\n",
    "\n",
    "def remove_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove inter-category page-duplicates and collapse intra-category duplicates\n",
    "    \"\"\"\n",
    "    nodes_to_remove = [node for node in tqdm(set(df[df.duplicated(\"title\")][\"title\"])) if\n",
    "                      len(set(df[df[\"title\"] == node][\"parent\"])) > 1]\n",
    "    \n",
    "    df = df[~df['title'].isin(nodes_to_remove)]\n",
    "    df = df.drop_duplicates(subset=\"title\", keep=\"first\")\n",
    "    return df\n",
    "    \n",
    "def uniform_page_and_edge_names(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Uniform the spelling and format of string in the title and edges column\n",
    "    \"\"\"\n",
    "    df['title'] = df['title'].str.lower()\n",
    "    df['edges'] = df['edges'].apply(lambda x: [re.sub(' ', '_', l.strip().lower()) for l in x])\n",
    "    return df\n",
    "\n",
    "def remove_edges_not_in_nodelist(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Remove all edges not in the title column from the lists of edges\n",
    "    \"\"\"\n",
    "    tqdm.pandas()\n",
    "    nodes = df[\"title\"].tolist()\n",
    "    df['edges'] = df['edges'].progress_apply(lambda x: [e for e in x if e in nodes])\n",
    "    return df\n",
    "\n",
    "# Calling the functions\n",
    "pd.options.mode.chained_assignment = None  # Hide Pandas Warnings\n",
    "df = remove_duplicates(pages_df)\n",
    "df = remove_anthro(df)\n",
    "df = uniform_page_and_edge_names(df)\n",
    "df = remove_edges_not_in_nodelist(df)\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further subset our data we remove self-loops and model a undirected network from which we extract the giant connected component. We then try to sample a respresntative supgraph containing $\\frac{1}{4}$ of the giant connected component's nodes using a Metropolis algoritm as stated in {cite}`Hbler2008` and implemented in the function `.MetropolisHastingsRandomWalkSampler()` from the module `littleballoffur`. The intuition behind the method is to initialise a random subgraph $S$ of our full network $G$ from which we iteratively remove and add nodes in order to mimic some topological properties of $G$, in this case the degree-distriubtion. This is of course not without consequences as we remove of the complexity in our network and thereby lose information, however we deem this nesecarry to reduce our network to a more manageble size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:26:07.936980Z",
     "start_time": "2021-12-05T10:25:44.976437Z"
    }
   },
   "outputs": [],
   "source": [
    "# We create a dictionary with the index each pagename\n",
    "index_dict = {i:k for k, i in enumerate(df['title'])}\n",
    "\n",
    "# We create an edgelist \n",
    "edge_list = []\n",
    "for node, edges in zip(df['title'].tolist(), df['edges'].tolist()):\n",
    "    for edge in edges:\n",
    "        edge_list.append((index_dict[node], index_dict[edge]))\n",
    "edge_list = [e for e in edge_list if e[0] != e[1]] #  ... and remove self-loops\n",
    "\n",
    "# We model the acutal graph and extract the gcc\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edge_list)\n",
    "gcc = max(nx.connected_components(G), key=len)\n",
    "G = G.subgraph(gcc)\n",
    "\n",
    "# littleballoffur requires all names to be translated into integers\n",
    "G = nx.relabel.convert_node_labels_to_integers(G)\n",
    "\n",
    "# We make a sample with 0.25 of the gcc's observations\n",
    "number_of_nodes = int(0.25 * G.number_of_nodes())\n",
    "sampler = MetropolisHastingsRandomWalkSampler(number_of_nodes = number_of_nodes)\n",
    "sampled_supgraph = sampler.sample(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To asses the bias induced by our sampling we compare the networks in terms of their degree exponent and transitivity. This way we can make sure that the networks degree distribution and level of clusterness is somewhat comparable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:26:34.707419Z",
     "start_time": "2021-12-05T10:26:07.941573Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Calculating the exponent\n",
    "G_degree_dist = [i[1] for i in G.degree]\n",
    "sampled_supgraph_degree_dist = [i[1] for i in sampled_supgraph.degree]\n",
    "G_exponent = powerlaw.Fit(G_degree_dist)\n",
    "sampled_supgraph_exponent = powerlaw.Fit(sampled_supgraph_degree_dist)\n",
    "\n",
    "# Calculating the transitivity\n",
    "G_transitivity = nx.transitivity(G)\n",
    "sampled_supgraph_transitivity = nx.transitivity(sampled_supgraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:26:34.789799Z",
     "start_time": "2021-12-05T10:26:34.750644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Degree distribution exponent full network G: 3.2974527129230946\n",
      "Degree distribution exponent supgraph S: 3.8342121796821265\n",
      "----------------------------------------------------------------------\n",
      "Transitivity full network G: 0.1929\n",
      "Transitivity supgraph S: 0.2991\n"
     ]
    }
   ],
   "source": [
    "print(f\"Degree distribution exponent full network G: {G_exponent.power_law.alpha}\")\n",
    "print(f\"Degree distribution exponent supgraph S: {sampled_supgraph_exponent.power_law.alpha}\")\n",
    "print(\"-\"*70)\n",
    "print('Transitivity full network G: {:.4f}'.format(transitivity))\n",
    "print('Transitivity supgraph S: {:.4f}'.format(transitivity_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reassuringly, the exponent of the two networks are fairly similiar suggesting that the degree distributions follows the somewhat same structure. A bit more worrying is the difference in transitivity that shows that we have oversestimated the transitivity in our supgraph. The result of this is that our network are more dense with higher number of \"triads\", than what really is the case. Nontheless we still decide to use the supgraph for our further analysis. We can now model our final network and corresponding DataFrame that we will use for our analysis based on the nodes from the supgraph. Finally we restrict this network to the weakly connected component as we create a *directed* network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:31:54.714195Z",
     "start_time": "2021-12-05T10:31:42.815008Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b226fb4ef1744fc9c07fb3752af44cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We define a list of nodes to keep and remove edges that refers to nodes that we have removed\n",
    "nodes_to_keep = [list(index_dict.keys())[i] for i in list(sampled_supgraph.nodes())]\n",
    "df = df[df['title'].isin(nodes_to_keep)].reset_index()[[\"title\", \"parent\", \"depth\", \"text\", \"edges\"]]\n",
    "df = remove_edges_not_in_nodelist(df)\n",
    "\n",
    "# Set attributes\n",
    "node_attr = df[[\"title\", \"parent\", \"depth\"]].set_index(\"title\").to_dict(orient='index')\n",
    "\n",
    "# Create an edgelist\n",
    "edge_list = []\n",
    "for node, edges in zip(df['title'].tolist(), df['edges'].tolist()):\n",
    "    for edge in edges:\n",
    "        edge_list.append((node, edge))\n",
    "\n",
    "# Model the network\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edge_list)\n",
    "nx.set_node_attributes(G, node_attr)\n",
    "\n",
    "# Extract the weakly connected component \n",
    "gcc = max(nx.weakly_connected_components(G), key=len)\n",
    "G = G.subgraph(gcc)\n",
    "\n",
    "# Add a gcc column to our final DataFrame\n",
    "df[\"gcc\"] = df[\"title\"].apply(lambda x: 1 if x in G.nodes() else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:33:01.844583Z",
     "start_time": "2021-12-05T10:33:01.813041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the edgelist as the a directed network can not be pickled\n",
    "with open(\"Final_edge_list.pickle\", 'wb') as f:\n",
    "    pickle.dump(edge_list, f)\n",
    "    \n",
    "# Saving the node attributes\n",
    "with open(\"Final_node_attr.pickle\", 'wb') as f:\n",
    "    pickle.dump(node_attr, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we conduct light preprocessing since the wikipedia documents are fairly clean. We do the following preprocessing steps:\n",
    "\n",
    "- **1 Getting the clean text from the Wikipedia pages:**\n",
    "The Wikipedia pages are structued in a page-specific text section and a section with links called \"See Also\". We are only intereseted in first one. We clean the text further by removing all non-alphanumerical characters.\n",
    "\n",
    "- **2 Remove stopwords:** \n",
    "To reduce the size of the pages vocabulary we remove all stopwords as they do not carry a lot of information relative to their size. \n",
    "\n",
    "- **3 Lemmatization of words:**\n",
    "Lemmatization refer to the reduction of words to its syntactical root to align words despite grammatical modifications which is useful for our analysis of text. We therefore add a column to our final DataFrame with lemmatized words.\n",
    "\n",
    "- **4 Tokenisation of words:**\n",
    "We split up all the lemmatized pages into list of words (or tokens) and add them to a new column in our final DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:34:17.791525Z",
     "start_time": "2021-12-05T10:33:21.487399Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d049402f0c64e3cabe5cbc7ad4a239e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f003087d33a4fc794ef030a91b2879a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04e50e22b5434f38b8c4c819f77437ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a74d52b4a12a4b55881a3fb8e7dab949",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Extracts clean text from the Wikipedia-pages by splitting\n",
    "    non-alphanumerical characters on the \"See also\" section and removing\n",
    "    \"\"\"\n",
    "    text = text.split('See also')[0]\n",
    "    text = re.sub('\\W+', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "def remove_stopwords(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Removes nltk stopwords bounded by whitespace and replace it\n",
    "    with whitespace.\n",
    "    \"\"\"\n",
    "    patterns = set(stopwords.words('english'))\n",
    "    for pattern in patterns:\n",
    "        if re.search(' '+pattern+' ', text):          \n",
    "            text = re.sub(' '+pattern+' ', ' ', text) \n",
    "    return text\n",
    "\n",
    "def lemmatize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Lemmatize all text.\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text) \n",
    "    sent_lemmatized = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return ' '.join(sent_lemmatized)\n",
    "\n",
    "def word_tokenize(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Tokenize all text.\n",
    "    \"\"\"\n",
    "    text = WordPunctTokenizer().tokenize(text)\n",
    "    return text\n",
    "\n",
    "# we apply all functions to the text\n",
    "warnings.filterwarnings('ignore')  # Ignore DepreciationError\n",
    "tqdm.pandas()\n",
    "df['cleaned_text'] = df['text'].astype(str).progress_apply(lambda x: clean_text(x))\n",
    "df['lemmatized'] = df['cleaned_text'].astype(str).progress_apply(lambda x: remove_stopwords(x))\n",
    "df['lemmatized'] = df['lemmatized'].astype(str).progress_apply(lambda x: lemmatize(x))\n",
    "df['tokens'] = df['lemmatized'].astype(str).progress_apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:35:35.933496Z",
     "start_time": "2021-12-05T10:35:31.836493Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save final DataFrame\n",
    "df.to_pickle(\"Final_df.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-05T10:59:06.000899Z",
     "start_time": "2021-12-05T10:59:05.914735Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>depth</th>\n",
       "      <th>text</th>\n",
       "      <th>edges</th>\n",
       "      <th>gcc</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parent</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>anthropology</th>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "      <td>510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>economics</th>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "      <td>1020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>political_science</th>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "      <td>1614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>psychology</th>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "      <td>543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sociology</th>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   title  depth  text  edges   gcc  cleaned_text  lemmatized  \\\n",
       "parent                                                                         \n",
       "anthropology         510    510   510    510   510           510         510   \n",
       "economics           1020   1020  1020   1020  1020          1020        1020   \n",
       "political_science   1614   1614  1614   1614  1614          1614        1614   \n",
       "psychology           543    543   543    543   543           543         543   \n",
       "sociology            615    615   615    615   615           615         615   \n",
       "\n",
       "                   tokens  \n",
       "parent                     \n",
       "anthropology          510  \n",
       "economics            1020  \n",
       "political_science    1614  \n",
       "psychology            543  \n",
       "sociology             615  "
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"parent\").count()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
