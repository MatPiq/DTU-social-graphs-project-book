{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(explainer_topic_model)=\n",
    "hSBM topic modelling\n",
    "=================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling with hSBM: Community Detection in a Topic Model Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we introduce part of our text analysis, namely the part which pertains to our topic modelling approach. Topic models are mainly used to cluster a collection of documents into different so-called 'topics'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant modules\n",
    "import joblib\n",
    "from hSBM_Topicmodel.sbmtm import sbmtm\n",
    "from matplotlib import pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.colors as mcolors\n",
    "import networkx as nx\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspired by the approach set forth by [Gerlach et al. 2018](https://www.science.org/doi/10.1126/sciadv.aaq1360), we show the hierarchical clustering of documents and words from our wikipedia dataset for all six scientific disciplines using a **hierarchical Stochastic Block Model** (hSBM). We only consider words that appear more than ten times in the text corpus and end up with 4810 articles or in our case, nodes. As opposed to other popular topic models such as LDA, we do not need to specify number of groups, levels or topics beforehand, since hSBM automatically detects these parameters. The model is inspired by community detection in networks and creates a bipartite-like network of words and documents. It splits the network into groups on different hierarchical levels organized as a tree. On each level, the hSBM clusters the documents and words in varying group sizes.\n",
    "\n",
    "We have fed the model with data consisting of preprocessed tokens from each Wikipedia article where wer remove infrequent tokens with a threshold at minimum 10 occurences per token. We are able to extract the documents from the model using the document number which corresponds to the index from our dataframe. We have merged the title of each article with the discipline from which it originates i.e. 'Anarchism' becomes 'political_science-anarchism', which we need for the forthcoming analysis. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "model = joblib.load(\"hSBM_model.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the structure of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the model\n",
    "model.plot(nedges=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment on the visualization\n",
    "\n",
    "And now for the real fun - let's extract the topics. We have experimented with extracting topics at different levels of the model (our model has 7 levels). Generally, we find that the topics at level 1 are the most cohesive and specific which is useful for our analysis. We specify that the model should return the 20 most probable words per topic (remember the model itself chooses number of topics). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_l1 = model.topics(l=1,n=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model generates 76 topics on level 1 which span from very method-related to more theme or topic-specific such as 'socialism' or 'US politics'. On level 2, we find 15 topics which are more discipline-related and broader. The lower the level, the less semantically cohesive the topics. We choose and name 10 topics for our content analysis: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the 10 topics from level 1 of our model\n",
    "chosen_topics = {'administrative_science':topics_l1[7], 'market_economy':topics_l1[18], 'museum_anthropology':topics_l1[23],\n",
    "  'cognitive_psychology':topics_l1[25], 'academia':topics_l1[30], 'statistical_methods':topics_l1[32], \n",
    "   'socialism':topics_l1[43], 'labour_economics':topics_l1[53], 'class_racialization':topics_l1[68], \n",
    "  'US_politics_SoMe':topics_l1[75]}\n",
    "\n",
    "# we create a pandas dataframe for our topics and topic words\n",
    "topic_names = list(chosen_topics.keys())\n",
    "topic_values = list(chosen_topics.values())\n",
    "\n",
    "topic_df = pd.DataFrame(index = topic_names)\n",
    "topic_df['topics'] = topic_values\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topics we chose are cherry picked meaning that we choose them based on word clusters we find interesting - both some which we believe to be 'pure' discipline and some which we believe to capture some overlap between disciplines. Next step is to extract the topic distribution for each chosen topic related to the articles which the given topic is most contributing in. Luckily, the hSBM library has a function we can use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get model paramers from level 1\n",
    "model_params = loaded_model.get_groups(l=1)\n",
    "\n",
    "# create a dataframe to store the topic distributions from the model parameters\n",
    "topic_dist_df = pd.DataFrame(model_params['p_tw_d'],\n",
    "columns=new_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(p_tw_d corresponds to probability of word group e.i. topic is in a given document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a new dict with original key values as keys and inferred topic as value\n",
    "new_keys_names = list(chosen_topics.keys())\n",
    "topic_num = [7, 18, 23, 25, 30, 32, 43, 53, 68, 75]\n",
    "topic_num_names = dict(zip(topic_num, new_keys_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to extract the topic distributions for each topic. We create a function which returns the topic name, the article and their probabilities as well as the words and word probabilities for each topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_dist(key):\n",
    "    topic = topics_l1[key]\n",
    "    words = [tuple[0] for index, tuple in enumerate(topic)]\n",
    "    word_prob = [tuple[1] for index, tuple in enumerate(topic)]\n",
    "    topic_dist = topic_dist_df.iloc[key].sort_values(ascending = False)[:30]\n",
    "    article_probabilities = [t for t in topic_dist]\n",
    "    articles = [t for t in topic_dist.index]\n",
    "    return topic_num_names[key], article_probabilities, articles, words, word_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we need now is to store the different values for our following analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lists of the different values we need \n",
    "topics = []\n",
    "probabilities = []\n",
    "article_names = []\n",
    "words = []\n",
    "word_prob = []\n",
    "\n",
    "# we use our topic distribution function to extract all relevant values\n",
    "for key in topic_num:\n",
    "    element = get_topic_dist(key)\n",
    "    topics.append(element[0])\n",
    "    probabilities.append(element[1])\n",
    "    article_names.append(element[2])\n",
    "    words.append(element[3])\n",
    "    word_prob.append(element[4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bipartite network of 10 topics-articles and 10 topic-words\n",
    "\n",
    "We want to visualize the topics and their related articles as well as their related words in two separate bipartite networks. The topics will be one type of nodes and the articles/words another type. Links represents articles/words connected to the given topic. The links are further weigthed by the probabilities for an article/word to contributing/explanatory of the topic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remember this is not a coding class xD\n",
    "y_word = list()\n",
    "w_word = list()\n",
    "\n",
    "# we create two different lists of tuples\n",
    "for i in range(len(topics)):\n",
    "    for element in range(len(words[0])):\n",
    "        # topic name and related words\n",
    "        y_word.append((topics[i], words[i][element]))\n",
    "        #topic name, words and their probabilities\n",
    "        w_word.append((topics[i], words[i][element],word_prob[i][element]))\n",
    "    \n",
    "# we repeat the process for articles     \n",
    "y = list()\n",
    "w = list()\n",
    "\n",
    "for i in range(len(article_names)):\n",
    "    for element in range(len(article_names[0])):\n",
    "        y.append((topics[i], article_names[i][element]))\n",
    "        w.append((topics[i], article_names[i][element],probabilities[i][element]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# two separate dataframes to create the network from \n",
    "bipart_nx = pd.DataFrame(w, columns = ['sender', 'receiver', 'weight'])\n",
    "word_nx = pd.DataFrame(w_word, columns = ['sender', 'receiver', 'weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bipartite network x graph object of topic-words\n",
    "B_word=nx.Graph()\n",
    "B_word.add_nodes_from(word_nx['receiver'], bipartite=0)\n",
    "B_word.add_nodes_from(word_nx['sender'], bipartite=1)\n",
    "B_word.add_weighted_edges_from(w_word)\n",
    "\n",
    "# bipartite network x graph object of topic-articles\n",
    "B=nx.Graph()\n",
    "B.add_nodes_from(bipart_nx['receiver'], bipartite=0)\n",
    "B.add_nodes_from(bipart_nx['sender'], bipartite=1)\n",
    "B.add_weighted_edges_from(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mangler\n",
    "\n",
    "* vise svg fil eller visualiser med netwulf \n",
    "* lav wordclouds \n",
    "* skriv content analysen ud fra visualiseringer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gerlach et al. point to the hSBM's ability to identify groups of stopwords. We however chose to remove stopwords in our initial preprocessing. We do not specify any stopwords for the wordclouds to bypass. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordcloud of Top 20 words in each topic\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  \n",
    "\n",
    "cloud = WordCloud(background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=20,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0)\n",
    "\n",
    "topics = topics_l1\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsample "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plots "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
