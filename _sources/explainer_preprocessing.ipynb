{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-30T09:45:17.299670Z",
     "start_time": "2021-11-30T09:45:17.282840Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we conduct light preprocessing since the wikipedia documents do not contain a lot of noise. \n",
    "In the following preprocessing steps, we split on the ‘see also’ section on each page to remove all possible irrelevant links. We remove all non-alphanumeric characters such as quotation marks, line break characters and remove punctuation. Finally, we lowercase before moving on to removing stopwords, lemmatizing and tokenizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions for preprocessing\n",
    "def clean_text(text):\n",
    "    # get relevant text (exclude references)\n",
    "    text = text.split('See also')[0]\n",
    "    # remove all non alphanumerical characters\n",
    "    text = re.sub('\\W+', ' ', text)\n",
    "    return text.lower()\n",
    "\n",
    "#Remove default stopwords\n",
    "def remove_stopwords(text): \n",
    "    patterns = set(stopwords.words('english'))\n",
    "    for pattern in patterns:\n",
    "        if re.search(' '+pattern+' ', text):           #Searching for stopwords bounded by whitespace in each tweet\n",
    "            text = re.sub(' '+pattern+' ', ' ', text)  #Substituting stopwords with whitespace\n",
    "    return text\n",
    "\n",
    "def lemmatize(text):\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    text = word_tokenize(text)      #Creating lemmatizer.\n",
    "                                        #Tokenizing, as lemmatizer only takes tokenized sentences\n",
    "    sent_lemmatized = []                    #Empty list to save lemmatized sentence\n",
    "\n",
    "    for word in text:\n",
    "        lemma = lemmatizer.lemmatize(word)  #Where the magic happens\n",
    "        sent_lemmatized.append(lemma)\n",
    "    \n",
    "    return ' '.join(sent_lemmatized)\n",
    "\n",
    "def word_tokenize(text):\n",
    "    text = WordPunctTokenizer().tokenize(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create three different columns of preprocessed text to work with to accommodate different needs for our text analysis methods:\n",
    "\n",
    "1. The first will simply contain the preprocessed text as described above. \n",
    "2. In the second stopwords have been removed and all words have been lemmatized. \n",
    "3. The final one is a duplicate of the second column in a tokenized version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we apply all functions to the text\n",
    "df['cleaned_text'] = df['text'].astype(str).apply(lambda x: clean_text(x)) #contains stopwords\n",
    "df['lemmatized'] = df['cleaned_text'].astype(str).apply(lambda x: remove_stopwords(x)) #removing stopwords\n",
    "df['lemmatized'] = df['lemmatized'].astype(str).apply(lambda x: lemmatize(x))\n",
    "df['tokens'] = df['lemmatized'].astype(str).apply(lambda x: word_tokenize(x))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
